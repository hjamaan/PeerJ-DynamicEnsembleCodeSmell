{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4WXA8pX2eQI"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "!pip install optuna\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, sys, string, time, math\n",
    "import optuna\n",
    "from   datetime import date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing  import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute         import SimpleImputer\n",
    "from imblearn.pipeline      import Pipeline, make_pipeline\n",
    "from scipy.stats            import wilcoxon\n",
    "from statistics             import mean\n",
    "\n",
    "from sklearn.ensemble     import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron\n",
    "from sklearn.svm          import SVC, LinearSVC\n",
    "from sklearn.naive_bayes  import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.tree         import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.neighbors    import KNeighborsClassifier\n",
    "from sklearn.neural_network           import MLPClassifier\n",
    "from sklearn.gaussian_process         import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "from sklearn.discriminant_analysis    import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics           import make_scorer, accuracy_score, f1_score, roc_curve, roc_auc_score, auc, brier_score_loss, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.model_selection   import cross_validate, cross_val_score, StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "with warnings.catch_warnings():\n",
    "  warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzD9CD2M4qpJ"
   },
   "outputs": [],
   "source": [
    "smells = ['[J] God Class.csv', '[J] Data Class.csv', '[J] Long Method.csv', '[J] Long Parameter List.csv', '[J] Feature Envy.csv', '[J] Switch Statements.csv' ,\n",
    "         '[P] Large Class.csv', '[P] Long Method.csv']\n",
    "\n",
    "fnames = smells\n",
    "\n",
    "models = {\n",
    "    'KNN' : KNeighborsClassifier(),\n",
    "    'DT'  : DecisionTreeClassifier(),\n",
    "    'LR'  : LogisticRegression(solver='liblinear',max_iter=500),\n",
    "    'SVM' : SVC(probability=True),\n",
    "    'MLP' : MLPClassifier(max_iter=500),\n",
    "    'NB'  : BernoulliNB(),\n",
    "}\n",
    "\n",
    "DetDF = pd.DataFrame (columns = ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'Brier', 'AUC', 'MCC', 'Time', 'Dataset'])\n",
    "ResDF = pd.DataFrame (columns = ['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1-score', 'Brier', 'AUC', 'MCC', 'Time', 'Dataset'])\n",
    "StaDF = pd.DataFrame (columns = ['Classifier_1', 'Classifier_2', 'Test', 'Stat', 'Sig-level', 'p-value', 'Null Hypo', 'Win', 'Lost', 'Effect Size', 'Effect Type', 'Dataset'])\n",
    "EnsDF = pd.DataFrame (columns = ['Ensemble', 'Base', 'Dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjc5qmmVrJMG"
   },
   "source": [
    "# **HP Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEohuPcKq_zq"
   },
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "n_jobs = -1\n",
    "cv = 5\n",
    "passthrough=False\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def tune_stack (model):\n",
    "  study = optuna.create_study(direction='maximize')\n",
    "  study.optimize(objective_STACK, n_trials=n_trials,n_jobs=n_jobs)\n",
    "  return StackingClassifier (estimators=get_estimators_pipelined(),\n",
    "                            final_estimator=LogisticRegression(solver='liblinear',**study.best_params),\n",
    "                            cv=cv, passthrough=passthrough, n_jobs=n_jobs)\n",
    "\n",
    "def tune_models ():\n",
    "  global models\n",
    "\n",
    "  tuned_models = {}\n",
    "  for key in models:\n",
    "    if 'DSE' not in key:\n",
    "      tuned_models[key] = tune_model(key,models[key])\n",
    "  models = tuned_models\n",
    "\n",
    "def tune_model(name,model):\n",
    "  study = optuna.create_study(direction='maximize')\n",
    "\n",
    "  if name == 'DT':\n",
    "    study.optimize(objective_DT, n_trials=n_trials,n_jobs=n_jobs)\n",
    "    return DecisionTreeClassifier(**study.best_params)\n",
    "\n",
    "  elif name == 'LR':\n",
    "    study.optimize(objective_LR, n_trials=n_trials,n_jobs=n_jobs)\n",
    "    return LogisticRegression(solver='liblinear',max_iter=1000,**study.best_params)\n",
    "\n",
    "  elif name == 'KNN':\n",
    "    study.optimize(objective_KNN, n_trials=n_trials,n_jobs=n_jobs)\n",
    "    return KNeighborsClassifier(**study.best_params)\n",
    "\n",
    "  elif name == 'SVM':\n",
    "    study.optimize(objective_SVM, n_trials=n_trials,n_jobs=n_jobs)\n",
    "    return SVC(probability=True,**study.best_params)\n",
    "\n",
    "  elif name == 'MLP':\n",
    "    study.optimize(objective_MLP, n_trials=n_trials,n_jobs=n_jobs)\n",
    "    return MLPClassifier(max_iter=1000,**study.best_params)\n",
    "\n",
    "  elif name == 'NB':\n",
    "    study.optimize(objective_NB, n_trials=n_trials,n_jobs=n_jobs)\n",
    "    return BernoulliNB(**study.best_params)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkLq6tpLUBzl"
   },
   "outputs": [],
   "source": [
    "def objective_DT(trial):\n",
    "  max_depth     = trial.suggest_int('max_depth', 2, 12)\n",
    "  splitter      = trial.suggest_categorical('splitter', ['best', 'random'])\n",
    "  max_features  = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "\n",
    "  model = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        splitter=splitter,\n",
    "        max_features=max_features\n",
    "  )\n",
    "\n",
    "  score = cross_val_score(create_pipeline('',model), X, y, scoring=mcc_scoring, cv=cv, n_jobs=n_jobs).mean()\n",
    "  return score\n",
    "\n",
    "def objective_LR(trial):\n",
    "  penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "  C       = trial.suggest_float('C', 0, 100)\n",
    "\n",
    "  model = LogisticRegression(\n",
    "        solver='liblinear',\n",
    "        penalty=penalty,\n",
    "        C=C,\n",
    "        max_iter=1000\n",
    "  )\n",
    "\n",
    "  score = cross_val_score(create_pipeline('',model), X, y, scoring=mcc_scoring, cv=cv, n_jobs=n_jobs).mean()\n",
    "  return score\n",
    "\n",
    "def objective_KNN(trial):\n",
    "  weights     = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "  metric      = trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski'])\n",
    "  n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n",
    "\n",
    "  model = KNeighborsClassifier(\n",
    "      weights=weights,\n",
    "      metric=metric,\n",
    "      n_neighbors=n_neighbors\n",
    "  )\n",
    "\n",
    "  score = cross_val_score(create_pipeline('',model), X, y, scoring=mcc_scoring, cv=cv, n_jobs=n_jobs).mean()\n",
    "  return score\n",
    "\n",
    "def objective_SVM(trial):\n",
    "  kernel = trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly', 'sigmoid'])\n",
    "  gamma  = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "  C      = trial.suggest_float('C', 0.001, 100)\n",
    "\n",
    "  model = SVC(\n",
    "      probability=True,\n",
    "      kernel=kernel,\n",
    "      gamma=gamma,\n",
    "      C=C\n",
    "  )\n",
    "\n",
    "  score = cross_val_score(create_pipeline('',model), X, y, scoring=mcc_scoring, cv=cv, n_jobs=n_jobs).mean()\n",
    "  return score\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def objective_MLP(trial):\n",
    "  activation = trial.suggest_categorical('activation', ['relu', 'identity', 'logistic','tanh'])\n",
    "  solver     = trial.suggest_categorical('solver', ['adam', 'lbfgs', 'sgd'])\n",
    "  alpha      = trial.suggest_float('alpha', 0.0001, 100)\n",
    "  learning_rate  = trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive'])\n",
    "\n",
    "  model = MLPClassifier(\n",
    "      max_iter=1000,\n",
    "      activation=activation,\n",
    "      solver=solver,\n",
    "      alpha=alpha,\n",
    "      learning_rate=learning_rate\n",
    "  )\n",
    "\n",
    "  score = cross_val_score(create_pipeline('',model), X, y, scoring=mcc_scoring, cv=cv, n_jobs=n_jobs).mean()\n",
    "  return score\n",
    "\n",
    "def objective_NB(trial):\n",
    "  alpha    = trial.suggest_float('alpha', 0.0001, 100)\n",
    "\n",
    "  model = BernoulliNB(\n",
    "      alpha=alpha,\n",
    "  )\n",
    "\n",
    "  score = cross_val_score(create_pipeline('',model), X, y, scoring=mcc_scoring, cv=cv, n_jobs=n_jobs).mean()\n",
    "  return score\n",
    "\n",
    "def objective_STACK(trial):\n",
    "  penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "  C       = trial.suggest_float('C', 0, 100)\n",
    "\n",
    "  model = StackingClassifier (estimators=get_estimators_pipelined(),\n",
    "                             final_estimator=LogisticRegression(solver='liblinear'),\n",
    "                             cv=cv, passthrough=passthrough, n_jobs=-1)\n",
    "\n",
    "  score = cross_val_score(create_pipeline('',model), X, y, scoring=mcc_scoring, cv=cv, n_jobs=n_jobs).mean()\n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Qw7SeUW2_Gu"
   },
   "source": [
    "# **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LFtA6DGSwRb"
   },
   "outputs": [],
   "source": [
    "def scoring_MCC (y_true, y_pred, **kwargs):\n",
    "  return matthews_corrcoef(np.array(y_true),np.array(y_pred))\n",
    "\n",
    "mcc_scoring = make_scorer(scoring_MCC, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4432kwwPsWyf"
   },
   "outputs": [],
   "source": [
    "def create_pipeline(name,model):\n",
    "\n",
    "  if 'DSE' in name: return model\n",
    "\n",
    "  steps = []\n",
    "  _miss  = True\n",
    "  _scale = True\n",
    "\n",
    "  if _miss:  steps.append(('impute',SimpleImputer(missing_values=np.nan, strategy='mean')))\n",
    "  if _scale: steps.append(('scale',MinMaxScaler()))\n",
    "\n",
    "  steps.append((name,model))\n",
    "\n",
    "  return Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DSWEFqWBjrM"
   },
   "source": [
    "# **Stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhSXCJ6cvsfK"
   },
   "outputs": [],
   "source": [
    "def save_stack_info (ensemble,estimators) :\n",
    "  global EnsDF\n",
    "\n",
    "  EnsDF = EnsDF.append({'Ensemble' : ensemble,\n",
    "                        'Base'     : ','.join([n for n,_ in estimators]),\n",
    "                        'Dataset'  : fname},\n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-kCdqEYovk2"
   },
   "outputs": [],
   "source": [
    "def create_stacking (estimators):\n",
    "\n",
    "  stack = StackingClassifier (estimators=estimators,\n",
    "                             final_estimator=LogisticRegression(solver='liblinear'),\n",
    "                             cv=cv, passthrough=passthrough, n_jobs=n_jobs)\n",
    "  stack = tune_stack (stack)\n",
    "\n",
    "  return stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9RoBGEZyFD1"
   },
   "outputs": [],
   "source": [
    "def get_estimators_pipelined ():\n",
    "\n",
    "  estimators = []\n",
    "  for key in models:\n",
    "    if 'DSE' not in key:\n",
    "      e = (key,create_pipeline(key,models[key]))\n",
    "      estimators.append(e)\n",
    "\n",
    "  return estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rsbvmc5B7_jW"
   },
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def evaluate_stacking (name,model):\n",
    "  score = cross_val_score(model, X, y, scoring=mcc_scoring, cv=cv,n_jobs=n_jobs).mean()\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggUJv_R7QTqq"
   },
   "outputs": [],
   "source": [
    "def full_stacking ():\n",
    "\n",
    "  estimators = get_estimators_pipelined ()\n",
    "  stack = create_stacking (estimators)\n",
    "  models['FSE'] = stack\n",
    "  save_stack_info ('FSE',estimators)\n",
    "\n",
    "  print('Full Stacking = \\n',EnsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxG4FRDZNCCP"
   },
   "outputs": [],
   "source": [
    "def forward_stacking ():\n",
    "\n",
    "  estimators = get_estimators_pipelined ()\n",
    "  candidate_estimators = estimators\n",
    "  selected_estimators = []\n",
    "  score = 0\n",
    "\n",
    "  while len(candidate_estimators) > 0:\n",
    "    selected = greedy_search_estimator (selected_estimators,candidate_estimators)\n",
    "    if selected is None: break\n",
    "    selected_estimators.append (selected)\n",
    "    candidate_estimators.remove (selected)\n",
    "\n",
    "  stack = create_stacking (selected_estimators)\n",
    "  models['DSE-GS'] = stack\n",
    "  save_stack_info ('DSE-GS',selected_estimators)\n",
    "\n",
    "  print('Forward Stacking Done = \\n',EnsDF)\n",
    "\n",
    "def greedy_search_estimator (selected_estimators,candidate_estimators):\n",
    "\n",
    "  candidate = None\n",
    "\n",
    "  if selected_estimators:\n",
    "    baseline = create_stacking (selected_estimators)\n",
    "    score = evaluate_stacking('DSE-GS',baseline)\n",
    "  else:\n",
    "    score = 0\n",
    "\n",
    "  for estimator in candidate_estimators:\n",
    "    duplicate = selected_estimators\n",
    "    duplicate.append(estimator)\n",
    "    baseline = create_stacking (duplicate)\n",
    "    new_score = evaluate_stacking('DSE-GS',baseline)\n",
    "\n",
    "    duplicate.remove(estimator)\n",
    "\n",
    "    if new_score > score:\n",
    "      score = new_score\n",
    "      candidate = estimator\n",
    "\n",
    "  return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPotyFmpSaGA"
   },
   "outputs": [],
   "source": [
    "def backward_stacking ():\n",
    "\n",
    "  full_estimators = get_estimators_pipelined ()\n",
    "  removed_estimators = []\n",
    "  score = 0\n",
    "\n",
    "  while len(full_estimators) >= 2:\n",
    "    selected = backward_elimination_estimator (full_estimators,removed_estimators)\n",
    "    if selected is None: break\n",
    "    removed_estimators.append (selected)\n",
    "    full_estimators.remove (selected)\n",
    "\n",
    "  stack = create_stacking (full_estimators)\n",
    "  models['DSE-BE'] = stack\n",
    "  save_stack_info ('DSE-BE',full_estimators)\n",
    "\n",
    "  print('Backward Stacking Done = \\n',EnsDF)\n",
    "\n",
    "def backward_elimination_estimator (full_estimators,removed_estimators):\n",
    "\n",
    "  candidate = None\n",
    "\n",
    "  if len(full_estimators) == 2: return None\n",
    "  baseline = create_stacking (full_estimators)\n",
    "  score = evaluate_stacking('DSE-BE',baseline)\n",
    "\n",
    "  for estimator in full_estimators:\n",
    "    duplicate = full_estimators.copy()\n",
    "    duplicate.remove(estimator)\n",
    "    baseline = create_stacking (duplicate)\n",
    "    new_score = evaluate_stacking('DSE-BE',baseline)\n",
    "\n",
    "    if new_score >= score:\n",
    "      score = new_score\n",
    "      candidate = estimator\n",
    "\n",
    "  return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Gn08DucBr_A"
   },
   "source": [
    "# **Main Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "mWNCxNP053hU",
    "outputId": "79bcde22-a28d-4bce-f20b-8485e8be81cb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fname in fnames:\n",
    "\n",
    "  dataset = pd.read_csv (fname)\n",
    "  dataset.drop_duplicates()\n",
    "  fname = fname.replace ('.csv','')\n",
    "  nunique  = dataset.apply(pd.Series.nunique)\n",
    "  colsDrop = nunique[nunique == 1].index\n",
    "  dataset  = dataset.drop(colsDrop, axis=1)\n",
    "\n",
    "  X = dataset.iloc[:, 0:-1]\n",
    "  y = dataset.iloc[:,-1]\n",
    "  y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "  tune_models()\n",
    "  full_stacking()\n",
    "  forward_stacking()\n",
    "  backward_stacking()\n",
    "\n",
    "  for name in models:\n",
    "\n",
    "    model = models[name]\n",
    "    folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=10)\n",
    "\n",
    "    scoring = {'Accuracy'  : 'accuracy',\n",
    "               'Precision' : 'precision',\n",
    "               'Recall'    : 'recall',\n",
    "               'F1-score'  : 'f1',\n",
    "               'Brier'  : 'neg_brier_score',\n",
    "               'AUC'    : 'roc_auc',\n",
    "               'MCC'    : mcc_scoring}\n",
    "\n",
    "    pipe_model = create_pipeline(name,model)\n",
    "    scores = cross_validate(pipe_model, X, y, scoring=scoring, cv=folds,n_jobs=-1)\n",
    "\n",
    "    acc_results = scores ['test_Accuracy']\n",
    "    pre_results = scores ['test_Precision']\n",
    "    rec_results = scores ['test_Recall']\n",
    "    f1s_results = scores ['test_F1-score']\n",
    "    bre_scores  = abs( scores ['test_Brier'] )\n",
    "    auc_scores  = scores ['test_AUC']\n",
    "    mcc_scores  = scores ['test_MCC']\n",
    "    time_scores = scores ['score_time']\n",
    "\n",
    "    ResDF = ResDF.append({'Classifier' : name,\n",
    "                          'Accuracy'   : np.round(acc_results.mean() * 100,2),\n",
    "                          'Precision'  : np.round(pre_results.mean() * 100,2),\n",
    "                          'Recall'     : np.round(rec_results.mean() * 100,2), \n",
    "                          'F1-score'   : np.round(f1s_results.mean() * 100,2),\n",
    "                          'Brier'      : np.round(abs(bre_scores.mean()),2),\n",
    "                          'AUC'        : np.round(auc_scores.mean(),2),\n",
    "                          'MCC'        : np.round(mcc_scores.mean(),2),\n",
    "                          'Time'       : np.round(time_scores.mean(),2),\n",
    "                          'Dataset'    : fname}\n",
    "                         ,ignore_index = True)\n",
    "\n",
    "    for i in range(0,len(mcc_scores)):\n",
    "      DetDF = DetDF.append({'Classifier' : name,\n",
    "                            'Accuracy'   : np.round(acc_results[i] * 100,2),\n",
    "                            'Precision'  : np.round(pre_results[i] * 100,2),\n",
    "                            'Recall'     : np.round(rec_results[i] * 100,2), \n",
    "                            'F1-score'   : np.round(f1s_results[i] * 100,2),\n",
    "                            'Brier'      : np.round(abs(bre_scores[i]),2),\n",
    "                            'AUC'        : np.round(auc_scores[i],2),\n",
    "                            'MCC'        : np.round(mcc_scores[i],2),\n",
    "                            'Time'       : np.round(time_scores[i],2),\n",
    "                            'Dataset'    : fname}\n",
    "                           ,ignore_index = True)\n",
    "\n",
    "    print(name)\n",
    "    print(ResDF)\n",
    "    print(DetDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJS_GMnwJX2o"
   },
   "source": [
    "# **Results analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQnks4TpJbH9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(style='ticks',palette='Set3')\n",
    "\n",
    "for fname in fnames:\n",
    "  fname = fname.replace ('.csv','')\n",
    "  f, ax = plt.subplots(figsize=(10, 10))\n",
    "  flierprops = dict(markerfacecolor='0.75', markersize=5, linestyle='none')\n",
    "  box = DetDF[DetDF['Dataset'] == fname]\n",
    "  sns.boxplot (x='Classifier',y='F1-score', data=box, flierprops=flierprops)\n",
    "  ax.set_xticklabels(ax.get_xticklabels(),rotation=30)\n",
    "  plt.ylabel ('F1-score', size=12)\n",
    "  plt.xlabel ('Classifier', size=12)\n",
    "  plt.title (fname, fontweight='bold',size=12)\n",
    "  plt.savefig( '_' + fname + '_boxplot.png')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq97LSxKYI2I"
   },
   "outputs": [],
   "source": [
    "names = DetDF['Classifier'].unique().tolist()\n",
    "\n",
    "alpha = 0.05\n",
    "alpha = alpha / (len(names)*(len(names)-1)/2)\n",
    "\n",
    "for fname in fnames:\n",
    "  fname = fname.replace ('.csv','')\n",
    "  statDet = DetDF[DetDF['Dataset'] == fname]\n",
    "  for i in range(len(names)):\n",
    "\n",
    "    name = names[i]\n",
    "    nestedNames = names[i+1:]\n",
    "\n",
    "    for nested in nestedNames:\n",
    "\n",
    "      model_1 =  statDet.loc[statDet['Classifier'] == name  ]['F1-score']\n",
    "      model_2 =  statDet.loc[statDet['Classifier'] == nested]['F1-score']\n",
    "\n",
    "      m1_score = model_1.mean()\n",
    "      m2_score = model_2.mean()\n",
    "\n",
    "      win, lost, effect_s, effect_t = '','','',''\n",
    "\n",
    "      test = 'Wilcoxon'\n",
    "      if m1_score == m2_score : stat, p = 1, 1\n",
    "      else: stat, p = wilcoxon (model_1, model_2)\n",
    "\n",
    "      if p > alpha: decision = 'Accept'\n",
    "      else:\n",
    "        decision = 'Reject'\n",
    "        if   m1_score > m2_score : win, lost = name, nested\n",
    "        else                     : win, lost = nested, name\n",
    "\n",
    "        effect_s = stat / math.sqrt (len(model_1))\n",
    "        if   effect_s < 0.3  : effect_t = 'small'\n",
    "        elif effect_s < 0.5  : effect_t = 'moderate'\n",
    "        elif effect_s >= 0.5 : effect_t = 'large'\n",
    "\n",
    "\n",
    "      StaDF = StaDF.append({'Classifier_1' : name,\n",
    "                            'Classifier_2' : nested,\n",
    "                            'Test'         : test,\n",
    "                            'Stat'         : stat,\n",
    "                            'Sig-level'    : alpha,\n",
    "                            'p-value'      : p,\n",
    "                            'Null Hypo'    : decision,\n",
    "                            'Win'          : win,\n",
    "                            'Lost'         : lost,\n",
    "                            'Effect Size'  : effect_s,\n",
    "                            'Effect Type'  : effect_t,\n",
    "                            'Dataset'    : fname}\n",
    "                          ,ignore_index=True)\n",
    "\n",
    "print(StaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpkEhI2nABp0"
   },
   "outputs": [],
   "source": [
    "StaDF.to_excel ('_StatisticalResults.xlsx', header='column_names')\n",
    "ResDF.to_excel ('_SummaryResults.xlsx', header='column_names')\n",
    "DetDF.to_excel ('_DetailedResults.xlsx', header='column_names')\n",
    "EnsDF.to_excel ('_StackEstimators.xlsx', header='column_names')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
